---
title: "Predict which Brand of Products Customers Prefer Report"
author: "Damsa Ioana"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document: 
    highlight: pygments
    number_sections: yes
    keep_md: yes
    theme: flatly
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: yes
  word_document:
    highlight: pygments
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# OBJECTIVES AND OVERVIEW

  For this task, based on the data that the marketing department collected, we have to predict our customers brand preferences regarding the laptop category for the incomplete data that was lest without the brand preference stated. 
  
  We need to conclude this prediction so that our company knows with which of the brands (Acer or Sony) to folow a collaborative agreement.
  
  In order to do this we will follow the data analysis and model building/ predicting flow by inspecting the data, preparing the data and perform any preprocessing necessary, do the feature selection, sample the data, build the models, perfect them and train them. Final step is applying the model on the incomplete data and predict the preference in laptop brand of our customers.
  
# Data visualization
  
```{r libraries, message=FALSE, warning=FALSE, include=FALSE}
library(caret)
library(ggplot2)
library(readr)
library(outliers)
library(OneR)
library(Hmisc)
library(C50)

#bringing the data
complete<-read.csv("CompleteResponses.csv")
res2 <- rcorr(as.matrix(complete))

res2
```
  After bringing our data in the R environment we procced in looking over the data types, variables and the general characteristics of our data set.
  
```{r summary and str, echo=TRUE, warning=FALSE}
summary(complete)

str(complete)

nrow(complete)#number of rows
```

  We can see that the variables that we have are all numeric numeric and taht we will have to change some of them to factors.We proceed to change the lable variable brand from numeric to factor and rename the levels from "1" and "0" to "Acer" for "0" and "Sony" for "1".
  
  Next step is visualizing the data through some plots in order to see the distribution, relationships and patterns that may appear. To achieve this we will be using the library ggplot. 
  
  
```{r plot salary hist, echo=FALSE}
ggplot(complete,aes(x=salary))+
                geom_histogram(bins = 20,color="black",fill="cyan2")+
                ggtitle("The Distribution of the Salary")+
                scale_x_continuous(name="Salary")
               

```
  As we can see from the histogram above the distribution of the salary values is not a Gaussian one (Nomrmal distribution). We might want to check if other features have a similar problem.

```{r plot credit hist, echo=FALSE}
ggplot(complete,aes(x=credit))+
                geom_histogram(bins = 20,color="black",fill="cyan2")+
                ggtitle("The Distribution of the Credit")+
                scale_x_continuous(name="Credit")
               
```

  After running the histogram for the credit and all the other features(age, education levels, type of car) we can conclude that our dataset doesen't have a normal distribution. We will proceed with our prediction process and talk about this at the end of this report.
    We proceed to check out the percentage of people that prefer each brand and how the brand related to other attributes.
    
    
 
```{r plot brand , echo=FALSE}
complete$Preferedbrand<-as.factor(complete$brand)
levels(complete$Preferedbrand) <- c("Acer","Sony")
ggplot(complete,aes(x=Preferedbrand, fill=Preferedbrand))+
                geom_bar()
               
```   
  From the histogram above we can conclude that out of the surveyed customers the majority prefered Sony. Let's see how the brand related with the salry and age of the customers.
  
```{r age salary, echo=FALSE}
ggplot(complete, aes(x=age, y=salary, col=Preferedbrand)) + 
  geom_point()
```
  Based on the plot above we can conclude that the age and salary do have an effect on the brand that our customers chose.
  After further visualization we have seen that the car type, credit and education level have none to very low effect on the brand chosen. We will keep this in mind when doing our feature selection.
  
# Data PreProcessing and Feature Selection

  We will start of by running a correlation matrix on our data and exclude any highly correlated features. The car and education elev are highly correlated (0.9963) so we will discard them both because of the high correlation and low significance seen by plotting the data. Also in the correlation matrix the salary seem that isn't correlated with the brand but we know it is based on our visualizations.
```{r corr mat, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
rm(Preferedbrand)
res2
```
```{r SUBSETTING, echo=T}
#removing car,elevels,credit attributes
complete <- complete[c(1,2,7)]
str(complete)
```


  Next we will transform the brand into a factor, and try to see if we have any missing values. Also we will proceed to normalize the salary and age, find and remove the outliers in the salary and  bin it.
```{r normalize,  echo = T, results = 'hide'}


#transforming brand to a factor and setting the levels

complete$brand<-as.factor(complete$brand)
levels(complete$brand) <- c("Acer","Sony")

#normalize the salary and age

normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
 
complete$salary<-normalize(complete$salary)
complete$age<-normalize(complete$age)

#finding missing values


sum(is.na(complete)) #no missing values


#finding outliers in salary

outlier_tfSalary = outlier(complete$salary,logical=TRUE)
sum(outlier_tfSalary)

#what were the ouliers in salary

find_outlierSalary = which(outlier_tfSalary==TRUE,arr.ind=TRUE)

#Removing the outliers in salary

complete= complete[-find_outlierSalary,]


#binning the salary

bin(
  complete$salary,
  nbins = 5, 
  labels = NULL, 
  method = c( "content")
)

```
  
  We can see that after we are done with the prerpocessing we have 3 features (salary,age and brand),age and salary are normalized and brand is a vector.
```{r complete pre, echo=FALSE}
str(complete)
```
  
# Building the Models

  For this task we will build 2 models: a C5.0 Decision Tree and a Random forest Models. To build them, partition the data and make the predictions we will use the CARET library.
  
## Partitioning the Data
  
  Before starting building the models we need to slit our data in 2 sets: the subset we will use for training the models and the testing set the one that we will use for testing the performance of our models.

```{r part, echo=TRUE}
#creating the numbers for each set with a 75 % ratio

set.seed(123)

inTrain<- createDataPartition(y= complete$brand,p=.75, list = FALSE) 

#partitioning the data into training and testing set

training <- complete[ inTrain,]
testing <- complete[-inTrain,]

#number of rows and the table head in each set
nrow(training)
head(training)
nrow(testing)
head(testing)

```

## Setting the Cross-Validation as Training Method

```{r train c}
ctrl <- trainControl(
                     method = "repeatedcv",
                     number = 10,
                     repeats = 3,
                     summaryFunction = multiClassSummary,
                     classProbs = TRUE
                     )
```

## C5.0 Decision tree

```{r train forest}
set.seed(123)
treeFit <- train(
                 brand~.,
                 data = training,
                 method = "C5.0", 
                 trControl=ctrl,
                 tuneLength = 3,
                )

treeFit
```


```{r predict and conf }
treeBrand <- predict(treeFit, newdata = testing)

#confusion matrix and the statistics

confusionMatrix(data= treeBrand, testing$brand)


```

## Random Forest Model

```{r random forest algo}
#setting the cross valdidation

ctrl <- trainControl(
                     method = "repeatedcv",
                     number = 10,
                     repeats = 3,
                     summaryFunction = multiClassSummary
                    )


#setting the manually set grid/ mtry
mtry <- c(2,1,3,5,12,32)
rfGrid <- expand.grid(.mtry=mtry)

set.seed(123)


#train the model

system.time(
          randomForestFit <- train(
                                   brand~.,
                                   data = training,
                                   preProcess = c("center"),
                                   method = "rf", 
                                   trControl=ctrl,
                                   tuneGrid=rfGrid,
                                   importance=T
                                  )
            )

randomForestFit
```
  
```{r rf pred y confu}
#PREDICT THE BRAND
rfBrand <- predict(randomForestFit, newdata = testing)

#computes class probabilities for the model
rfProbs <- predict(randomForestFit, newdata = testing, type = "prob")
rfProbs

#confusion matrix for the random forest model
confusionMatrix(data= rfBrand, testing$brand)


#importance

plot(varImp(randomForestFit))

```
  
## Comparing the Models
  
```{r compa }
resamps <- resamples(list(tree = treeFit, forest = randomForestFit))
summary(resamps)

#visualize the comparation 
xyplot(resamps, what = "BlandAltman")

dotplot(resamps)

bwplot(resamps)

#For each resample, there are paired results a paired tâ€“test can be used to assess whether there
#is a difference in the average resampled area under the ROC curve

diffs <- diff(resamps)
summary(diffs)
plot.train(treeFit)
#rf better

```

# Predicting the Preffered Brand
